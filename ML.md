# 第1周

## 一、引言

### 1.3 监督学习

监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价。

然后运用学习算法，算出更多的正确答案。比如你朋友那个新房子的价格。用术语来讲，这叫做回归问题。我们试着推测出一个连续值的结果，即房子的价格。

### 1.4 无监督学习

无监督学习，它是学习策略，交给算法大量的数据，并让算法为我们从数据中找出某种结构。

## 二、单变量线性回归

### 2.1 模型表示

![image-20240617141528854](D:\cs_note\python\assets\image-20240617141528854.png)

![image-20240617141512847](D:\cs_note\python\assets\image-20240617141512847.png)

### 2.2 代价函数

![image-20240617141610367](D:\cs_note\python\assets\image-20240617141610367.png)

### 2.5 梯度下降

梯度下降背后的思想是：开始时我们随机选择一个参数的组合$\theta_0,\theta_1......$，计算代价函数，然后我们寻找下一个能让代价函数值**下降最多**的参数组合。我们持续这么做直到到到一个局部最小值。

因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值，选择不同的初始参数组合，可能会找到不同的局部最小值。

![image-20240617142048200](D:\cs_note\python\assets\image-20240617142048200.png)

其中$\alpha$是学习率，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数**减去学习速率乘以代价函数的导数**。

# 第2周

## 四、多变量线性回归

### 4.3 梯度下降法实践1-特征缩放

以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。

![img](http://www.ai-start.com/ml2014/images/966e5a9b00687678374b8221fdd33475.jpg)

解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。

![image-20240617142805725](D:\cs_note\python\assets\image-20240617142805725.png)

# 第3周

## 六、逻辑回归

### 6.1 分类问题

我们将因变量可能属于的两个类分别称为负向类和正向类，其中 0 表示负向类，1 表示正向类。

如果我们要用线性回归算法来解决一个分类问题，对于分类， 取值为 0 或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签 都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。

所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的**输出值永远在0到 1 之间**。

### 6.2 假说表示

![image-20240617143354223](D:\cs_note\python\assets\image-20240617143354223.png)

![img](http://www.ai-start.com/ml2014/images/1073efb17b0d053b4f9218d4393246cc.jpg)

### 6.4 代价函数

y=1，预测值越接近1，loss越小

y=0，预测值越接近0，loss越小

![image-20240617143509183](D:\cs_note\python\assets\image-20240617143509183.png)

![image-20240617143835612](D:\cs_note\python\assets\image-20240617143835612.png)

## 七、正则化

### 7.1 过拟合的问题

1. 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如**PCA**）
2. 正则化。 保留所有的特征，但是减少参数的大小。

### 7.2 代价函数

![image-20240617144318421](D:\cs_note\python\assets\image-20240617144318421.png)

# 第4周